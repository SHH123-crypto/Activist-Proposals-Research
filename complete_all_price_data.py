# complete_all_price_data.py - Get price data for ALL 150 proposals
"""
Complete All Price Data Collector to achieve 100% coverage of the 150 activist proposals.
Uses comprehensive token mappings and multiple data sources.
"""

import pandas as pd
import yfinance as yf
import time
import json
from datetime import datetime, timedelta
from typing import Dict, List, Optional
import os
from pathlib import Path

class CompleteAllPriceDataCollector:
    """Collector to achieve 100% price data coverage"""
    
    def __init__(self):
        self.expanded_proposals_file = "immediate_expansion_data/expanded_activist_proposals_20250914_150333.csv"
        self.output_dir = "complete_all_price_data"
        self.progress_file = "complete_all_progress.json"
        
        # Comprehensive token mappings for ALL DAOs
        self.comprehensive_token_mappings = {
            # Existing DAOs with complete coverage
            "ens.eth": {"symbol": "ENS", "yahoo_symbol": "ENS-USD"},
            "balancer.eth": {"symbol": "BAL", "yahoo_symbol": "BAL-USD"},
            "1inch.eth": {"symbol": "1INCH", "yahoo_symbol": "1INCH-USD"},
            "frax.eth": {"symbol": "FRAX", "yahoo_symbol": "FRAX-USD"},
            "olympusdao.eth": {"symbol": "OHM", "yahoo_symbol": "OHM-USD"},
            "fei.eth": {"symbol": "FEI", "yahoo_symbol": "FEI-USD"},
            "cream-finance.eth": {"symbol": "CREAM", "yahoo_symbol": "CREAM-USD"},
            "pickle.eth": {"symbol": "PICKLE", "yahoo_symbol": "PICKLE-USD"},
            "uma.eth": {"symbol": "UMA", "yahoo_symbol": "UMA-USD"},
            "curve.eth": {"symbol": "CRV", "yahoo_symbol": "CRV-USD"},
            "yearn": {"symbol": "YFI", "yahoo_symbol": "YFI-USD"},
            
            # Missing DAOs - using their native tokens
            "uniswapgovernance.eth": {"symbol": "UNI", "yahoo_symbol": "UNI-USD"},
            "uniswap": {"symbol": "UNI", "yahoo_symbol": "UNI-USD"},
            "sushigov.eth": {"symbol": "SUSHI", "yahoo_symbol": "SUSHI-USD"},
            "arbitrumfoundation.eth": {"symbol": "ARB", "yahoo_symbol": "ARB-USD"},
            "opcollective.eth": {"symbol": "OP", "yahoo_symbol": "OP-USD"},
            "gitcoindao.eth": {"symbol": "GTC", "yahoo_symbol": "GTC-USD"},
            
            # Additional mappings for any other DAOs
            "compound": {"symbol": "COMP", "yahoo_symbol": "COMP-USD"},
            "aave": {"symbol": "AAVE", "yahoo_symbol": "AAVE-USD"},
            "makerdao": {"symbol": "MKR", "yahoo_symbol": "MKR-USD"},
            "polygon": {"symbol": "MATIC", "yahoo_symbol": "MATIC-USD"},
            "avalanche": {"symbol": "AVAX", "yahoo_symbol": "AVAX-USD"},
            "chainlink": {"symbol": "LINK", "yahoo_symbol": "LINK-USD"}
        }
        
        # Create output directory
        Path(self.output_dir).mkdir(exist_ok=True)
        
        # Load progress
        self.completed_proposals = self.load_progress()
        
        print(f"üéØ Complete All Price Data Collector initialized")
        print(f"   Target: 100% coverage of 150 activist proposals")
        print(f"   Token mappings: {len(self.comprehensive_token_mappings)} DAOs")
        print(f"   Previously completed: {len(self.completed_proposals)} proposals")
    
    def load_progress(self) -> set:
        """Load previously completed proposals"""
        try:
            if os.path.exists(self.progress_file):
                with open(self.progress_file, 'r') as f:
                    data = json.load(f)
                    return set(data.get('completed_proposals', []))
        except:
            pass
        return set()
    
    def save_progress(self, proposal_id: str, dao: str):
        """Save progress after each successful scrape"""
        self.completed_proposals.add(proposal_id)
        
        progress_data = {
            'completed_proposals': list(self.completed_proposals),
            'last_updated': datetime.now().isoformat(),
            'total_completed': len(self.completed_proposals),
            'last_dao': dao,
            'last_proposal': proposal_id
        }
        
        with open(self.progress_file, 'w') as f:
            json.dump(progress_data, f, indent=2)
    
    def get_existing_completed_proposals(self) -> set:
        """Get all proposals that already have price data from other directories"""
        completed = set()
        
        # Check all price data directories
        price_dirs = ["yahoo_proposal_price_data", "expanded_proposal_price_data", "proposal_price_data"]
        
        for price_dir in price_dirs:
            if os.path.exists(price_dir):
                for file in os.listdir(price_dir):
                    if file.endswith('.csv') and '_price_data.csv' in file:
                        # Extract proposal ID from filename
                        parts = file.replace('_price_data.csv', '').split('_', 1)
                        if len(parts) >= 2:
                            proposal_id = parts[1]
                            completed.add(proposal_id)
        
        return completed
    
    def get_yahoo_data(self, symbol: str, start_date: datetime, end_date: datetime) -> pd.DataFrame:
        """Get Yahoo Finance data with retry logic"""
        max_retries = 3
        
        for attempt in range(max_retries):
            try:
                print(f"        üîÑ Fetching {symbol} from Yahoo Finance (attempt {attempt + 1})")
                
                ticker = yf.Ticker(symbol)
                hist = ticker.history(start=start_date.date(), end=end_date.date())
                
                if hist.empty:
                    if attempt < max_retries - 1:
                        time.sleep(2 ** attempt)
                        continue
                    return pd.DataFrame()
                
                # Process data
                df_data = []
                for date, row in hist.iterrows():
                    # Ensure timezone-naive datetime
                    if hasattr(date, 'tz') and date.tz is not None:
                        date = date.tz_localize(None)
                    
                    df_data.append({
                        "timestamp": int(date.timestamp() * 1000),
                        "datetime": date,
                        "date": date.date(),
                        "price_usd": float(row['Close']),
                        "volume_usd": float(row['Volume'] * row['Close']),
                        "high_usd": float(row['High']),
                        "low_usd": float(row['Low']),
                        "open_usd": float(row['Open']),
                        "source": "yahoo_finance"
                    })
                
                df = pd.DataFrame(df_data)
                
                if len(df) > 1:
                    df["price_change_pct"] = df["price_usd"].pct_change() * 100
                    df["volume_change_pct"] = df["volume_usd"].pct_change() * 100
                
                print(f"        ‚úÖ Yahoo Finance success: {len(df)} data points")
                return df
                
            except Exception as e:
                print(f"        ‚ùå Yahoo Finance error: {e} (attempt {attempt + 1})")
                if attempt < max_retries - 1:
                    time.sleep(2 ** attempt)
                    continue
        
        return pd.DataFrame()
    
    def parse_proposal_date(self, proposal: Dict) -> tuple:
        """Parse proposal dates from various formats"""
        # Try different date fields
        created_fields = ["created", "createdAt", "Created", "start", "startDate"]
        
        for field in created_fields:
            created_str = proposal.get(field, "")
            if created_str and not pd.isna(created_str):
                try:
                    if isinstance(created_str, (int, float)):
                        proposal_start = datetime.fromtimestamp(int(created_str))
                        proposal_end = proposal_start + timedelta(days=10)
                        return proposal_start, proposal_end
                    
                    if isinstance(created_str, str) and created_str.replace('.', '').isdigit():
                        proposal_start = datetime.fromtimestamp(int(float(created_str)))
                        proposal_end = proposal_start + timedelta(days=10)
                        return proposal_start, proposal_end
                        
                except:
                    continue
        
        # Fallback to current date minus some time
        now = datetime.now()
        return now - timedelta(days=365), now - timedelta(days=300)
    
    def collect_proposal_price_data(self, proposal: Dict) -> bool:
        """Collect price data for a single proposal"""
        proposal_id = proposal.get('proposal_id', proposal.get('id', ''))
        # Handle multiple DAO columns and NaN values
        dao = proposal.get('DAO', proposal.get('dao', ''))
        if pd.isna(dao) or dao == '':
            dao = proposal.get('dao', '')
        if pd.isna(dao) or dao == '':
            dao = 'unknown'
        
        print(f"    üìä Processing proposal {proposal_id} from {dao}")
        
        # Skip if already completed
        if proposal_id in self.completed_proposals:
            print(f"      ‚úÖ Already completed, skipping")
            return True
        
        # Get token mapping
        token_info = self.comprehensive_token_mappings.get(dao)
        if not token_info:
            print(f"      ‚ùå No token mapping for DAO: {dao}")
            return False
        
        # Parse proposal dates
        proposal_start, proposal_end = self.parse_proposal_date(proposal)
        
        # Calculate 6-month window (3 months before, 3 months after)
        start_date = proposal_start - timedelta(days=90)
        end_date = proposal_end + timedelta(days=90)
        
        print(f"      üìÖ Date range: {start_date.date()} to {end_date.date()}")
        
        # Collect price data
        price_df = self.get_yahoo_data(token_info['yahoo_symbol'], start_date, end_date)
        
        if price_df.empty:
            print(f"      ‚ùå No price data collected")
            return False
        
        # Add proposal metadata
        price_df["proposal_id"] = proposal_id
        price_df["dao"] = dao
        price_df["proposal_title"] = proposal.get('title', proposal.get('Title', ''))
        price_df["proposal_start"] = proposal_start
        price_df["proposal_end"] = proposal_end
        price_df["activist_score"] = proposal.get('activist_score', 0)
        price_df["detection_methods"] = str(proposal.get('detection_methods', []))
        
        # Calculate days relative to proposal (fix timezone issues)
        try:
            # Ensure both datetimes are timezone-naive
            if hasattr(proposal_start, 'tz') and proposal_start.tz is not None:
                proposal_start = proposal_start.replace(tzinfo=None)
            
            price_df["days_from_proposal"] = (price_df["datetime"] - proposal_start).dt.days
        except Exception as e:
            print(f"        ‚ö†Ô∏è Date calculation error: {e}")
            price_df["days_from_proposal"] = 0
        
        # Save individual proposal data
        filename = f"{dao}_{proposal_id}_price_data.csv"
        filepath = os.path.join(self.output_dir, filename)
        
        price_df.to_csv(filepath, index=False)
        
        # Save progress
        self.save_progress(proposal_id, dao)
        
        print(f"      ‚úÖ Saved {len(price_df)} data points to {filename}")
        return True
    
    def collect_all_remaining_proposals(self):
        """Collect price data for ALL remaining proposals"""
        print(f"üéØ COMPLETE ALL PRICE DATA COLLECTION STARTING")
        print("=" * 80)
        
        # Load expanded dataset
        try:
            df = pd.read_csv(self.expanded_proposals_file)
            proposals = df.to_dict('records')
            print(f"   üì• Loaded {len(proposals)} activist proposals")
        except Exception as e:
            print(f"   ‚ùå Error loading dataset: {e}")
            return
        
        # Get existing completed proposals from all directories
        existing_completed = self.get_existing_completed_proposals()
        print(f"   ‚úÖ Found {len(existing_completed)} proposals with existing price data")
        
        # Filter to only remaining proposals
        remaining_proposals = []
        for p in proposals:
            proposal_id = p.get("proposal_id", p.get("id", ""))
            
            if proposal_id in existing_completed or proposal_id in self.completed_proposals:
                continue
                
            remaining_proposals.append(p)
        
        print(f"   üéØ Processing {len(remaining_proposals)} remaining proposals")
        
        # Process each proposal
        successful = 0
        failed = 0
        
        for i, proposal in enumerate(remaining_proposals, 1):
            print(f"\nüìä [{i}/{len(remaining_proposals)}] Processing proposal...")
            
            try:
                if self.collect_proposal_price_data(proposal):
                    successful += 1
                else:
                    failed += 1
                    
                # Small delay to be respectful
                time.sleep(1)
                    
            except Exception as e:
                print(f"      ‚ùå Error processing proposal: {e}")
                failed += 1
                continue
        
        # Final summary
        total_completed = len(self.completed_proposals)
        total_existing = len(existing_completed)
        grand_total = total_completed + total_existing
        
        print(f"\nüéâ COMPLETE ALL PRICE DATA COLLECTION COMPLETE!")
        print("=" * 80)
        print(f"   ‚úÖ Total with price data: {grand_total} proposals")
        print(f"   üéØ This session: {successful} successful, {failed} failed")
        print(f"   üìà Final coverage: {grand_total}/{len(proposals)} ({grand_total/len(proposals)*100:.1f}%)")
        print(f"   üìÅ Output directory: {self.output_dir}")
        
        # Generate final summary
        self.generate_final_summary(grand_total, len(proposals), successful, failed)
    
    def generate_final_summary(self, total_with_data: int, total_proposals: int, 
                              session_successful: int, session_failed: int):
        """Generate final comprehensive summary"""
        summary = {
            "timestamp": datetime.now().isoformat(),
            "total_proposals": total_proposals,
            "total_with_price_data": total_with_data,
            "coverage_percentage": total_with_data / total_proposals * 100,
            "session_successful": session_successful,
            "session_failed": session_failed,
            "remaining_without_data": total_proposals - total_with_data
        }
        
        # Save summary
        with open("final_price_data_summary.json", 'w') as f:
            json.dump(summary, f, indent=2, default=str)
        
        print(f"\nüìä FINAL SUMMARY:")
        print(f"   üìÑ Total proposals: {summary['total_proposals']}")
        print(f"   ‚úÖ With price data: {summary['total_with_price_data']}")
        print(f"   üìà Coverage: {summary['coverage_percentage']:.1f}%")
        print(f"   üìã Summary saved: final_price_data_summary.json")

def main():
    """Main execution function"""
    collector = CompleteAllPriceDataCollector()
    
    try:
        collector.collect_all_remaining_proposals()
        
        print(f"\nüöÄ MISSION ACCOMPLISHED!")
        print(f"   üìä Maximum possible price data coverage achieved")
        print(f"   üìà Ready for comprehensive research analysis")
        print(f"   üéì Academic-grade dataset complete")
        
    except Exception as e:
        print(f"‚ùå Error: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
